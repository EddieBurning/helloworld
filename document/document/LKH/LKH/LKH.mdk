Title         : LKH research
Author        : Eddie
Logo          : True

[TITLE]
It has been proven that TSP is a member of the set of NP-complete problems.
This is a class of difficult problems whose time complexity is probably exponential.
The members of the class are related so that if a polynomial time were
found for one problem, polynomial time algorithms would exist for all of
them. However, it is commonly believed that no such polynomial algorithm
exists. **Therefore, any attempt to construct a general algorithm for finding optimal
solutions for the TSP in polynomial time must (probably) fail**.

That is, **for any such algorithm it is possible to construct problem instances
for which the execution time grows at least exponentially with the size of the
input**. Note, however, that time complexity here refers to any algorithm’s behavior
**in worst cases**. It can not be excluded that there exist algorithms whose
average running time is polynomial. The existence of such algorithms is still
an open question.
# algorithms to solve the TSP
## Exact Algorithm

### Cutting-plane or facet-finding algorithm
  These algorithms are quite complex, with codes on the order
of 10,000 lines. In addition, the algorithms are very demanding of computer
power. For example, the exact solution of a symmetric problem with 2392
cities was determined over a period of more than 27 hours on a powerful super
computer [7]. It took roughly 3-4 years of CPU time on a large network
of computers to determine the exact solution of the previously mentioned
7397-city problem [8].

## Approximate Algorithm(or heuristic) algorithm 
In contrast, the approximate algorithms obtain good solutions but do not guarantee
that optimal solutions will be found. **These algorithms are usually very
simple and have (relative) short running times**. Some of the algorithms give
solutions that in average differs only by a few percent from the optimal solution.
Therefore, if a small deviation from optimum can be accepted, it may
be appropriate to use an approximate algorithm.

The class of approximate algorithms may be subdivided into the following
three classes:

* **Tour construction algorithms** gradually build a tour by adding a new city at each step.
 * **nearest neighbor algorithm& its variants:**Start in an arbitrary city. As long as there are cities,
that have not yet been visited, visit the nearest city that still has not appeared
in the tour. Finally, return to the first city.  **too greedy is not good!!!**

* **Tour improvement algorithms** improve upon a tour by performing various exchanges.
 * **K-opt move algorithm(LKH improved on this algorithm)**: hard to implement.
* **Composite algorithms**. The composite algorithms combine these two
features.



### branch and bound

It is a clever way enumerating all the solutions of an integer program to solve it more
efficiently. The trick is to use the **LP relaxation to bound the optimal integer solutions**
in a subtree of the enumeration tree, which allows us to eliminate many (in some cases 
even 99.999999999%) of the enumeration tree. 
``` javascript
While there remain active nodes
   Select an active node j and mark it as inactive
   Let x(j) and zLP(j) denote the optimal solution and
   objective of the LP relaxation of Problem(j).
  Case 1: If z* ≥ zLP(j) then
  Prune node j
  Case 2: If z* < zLP(j) and x(j) is feasible for IP then
  Replace the incumbent by x(j)
  Prune node j
  Case 3: If z* < zLP(j) and x(j) is not feasible for IP then
  Mark the direct descendants of node j as active
End While 
```


### 


# The Lin-Kernighan algorithm

The 2-opt algorithm is a special case of the &lambda;-opt algorithm [15], where in
each step &lambda; links of the current tour are replaced by &lambda; links in such a way that
a shorter tour is achieved. In other words, in each step a shorter tour is obtained
by **deleting &lambda; links and putting the resulting paths together in a new
way, possibly reversing one ore more of them**.

~ Lemma
A tour is said to be -optimal (or simply -opt) if it is impossible
to obtain a shorter tour by replacing any &lambda; of its links by any other
set of &lambda; links.
~

However, it is a drawback that &lambda; must be specified in advance. It is difficult
to know what &lambda; to use to achieve the best compromise between running time
and quality of solution.


Lin and Kernighan removed this drawback by introducing a powerful variable
&lambda;-opt algorithm. The algorithm changes the value of &lambda; during its execution,
deciding at each iteration what the value of &lambda; should be. At each iteration step
the algorithm examines, for ascending values of &lambda;, whether an interchange of
&lambda; links may result in a shorter tour. Given that the exchange of r links is being
considered, a series of tests is performed to determine whether r+1 link
exchanges should be considered. This continues until some stopping
conditions are satisfied.



## minimum 1-tree
**an alternative formulation of the symmetric TSP is: “Find a minimum 1-tree all whose nodes have degree 2”.**

Usually a minimum spanning tree contains many edges in common with an
optimal tour. An optimal tour normally contains between 70 and 80 percent of
the edges of a minimum 1-tree. Therefore, minimum 1-trees seem to be well
suited as a heuristic measure of ‘nearness’.**Edges that belong, or ‘nearly belong',
to a minimum 1-tree, stand a good chance of also belonging to an optimal
tour. Conversely, edges that are ‘far from’ belonging to a minimum 1-
tree have a low probability of also belonging to an optimal tour**. In the Lin-
Kernighan algorithm these ‘far’ edges may be excluded as candidates to enter
a tour. It is expected that this exclusion does not cause the optimal tour to be
missed.
~ Lemma
Let T be a minimum 1-tree of length L(T) and let T+(i,j) denote
a minimum 1-tree required to contain the edge (i,j). Then the
nearness of an edge (i,j) is defined as the quantity.


   &alpha; (i,j) = L(T+(i,j)) - L(T)
~

The &alpha;-measure can be used to systematically identify those edges that could
conceivably be included in an optimal tour, and disregard the remainder.
These 'promising edges', called the candidate set, may, for example, consist
of the k &alpha;-nearest edges incident to each node, and/or those edges having an
&alpha;-nearness below a specified upper bound.

In general, using the &alpha;-measure for specifying the candidate set is much better
than using nearest neighbors. Usually, the candidate set may be smaller,
without degradation of the solution quality.


## improved &alpha;-nearness
for the 532-city problem the worst case is an optimal edge being the 22nd 
c-nearest edge for a node, whereas the worst case when using the &alpha;-measure
is an optimal edge being the 14th a-nearest. The average rank of the optimal
edges among the candidate edges is reduced from 2.4 to 2.1.

This seems to be quite satisfactory. However, the &alpha;-measure can be improved
substantially by making a simple transformation of the original cost matrix.
The transformation is based on the following observations:

* Every tour is a 1-tree. Therefore the length of a minimum 1-tree
is a lower bound on the length of an optimal tour.
* If the length of all edges incident to a node are changed with the
same amount, p, any optimal tour remains optimal. Thus, if the
cost matrix C= (cij) is transformed to D = (dij), where
~ Center
d~ij~=c~ij~+&pi;~i~+&pi;~j~
~
**then an optimal tour for the D is also an optimal tour for C**.
**The length of every tour is increased by 2Spi. The transformation
leaves the TSP invariant, but usually changes the minimum 1-tree**.

* If T~&pi;~ is a minimum 1-tree with respect to D, then its length, L(T~&pi;~),
is a lower bound on the length of an optimal tour for D.
Therefore w(&pi;) = L(T~&pi;~) - 2&sum;&pi;~i~ is lower bound on the length of
an optimal tour for C.


**The aim is now to find a transformation, C&rArr;D, given by the vector
&pi; = (&pi;~1~,&pi;~2~, ...,&pi;~n~), that _maximizes the lower bound_ w(&pi;) = L(T~&pi;~) -2&sum;&pi;~i~ .**


If T~&pi;~ becomes a tour, then the exact optimum has been found. Otherwise, it
appears, at least intuitively, that if w(&pi;) > w(0), then a-values computed from
D are better estimates of edges being optimal than &alpha;-values computed from C.

Usually, the maximum of w(&pi;) is close to the length of an optimal tour. Computational
experience has shown that this maximum typically is less than 1
percent below optimum. However, finding maximum for w(&pi;) is not a trivial
task. The function is **piece-wise linear and concave, and therefore not differentiable
everywhere.**

**A suitable method for maximizing w(&pi;) is subgradient optimization** subgradient 
is a generalization of the gradient concept). It is an iterative
method in which the maximum is approximated by stepwise changes of p.
At each step p is changed in the direction of the subgradient, i.e., p~k+1~ = p~k~ +
t^k^v^k^, **where v^k^ is a subgradient vector, and t^k^ is a positive scalar, called the
step size**.

For the actual maximization problem it can be shown that v^k^ = d~k~ - 2 is a subgradient
vector, where d~k~ is a vector having as its elements the degrees of the
nodes in the current minimum 1-tree. This subgradient makes the algorithm
strive towards obtaining minimum 1-trees with node degrees equal to 2, i.e.,
minimum 1-trees that are tours. Edges incident to a node with degree 1 are
made shorter. Edges incident to a node with degree greater than 2 are made
longer. Edges incident to a node with degree 2 are not changed.


The &pi-values are often called penalties. The determination of a (good) set of
penalties is called an ascent.


### subgradient algorithm for computing an approximation W for the maximum of w(&pi;).

1. Let k = 0, p0 = 0 and W = -&infin;.
2. Find a minimum 1-tree, T~&pi;~k
3. Compute w(&pi;~k) = L(T&pi;k) - 2&sum;&pi;~i~
4. Let W = max(W, w(&pi;~k~).
5. Let v^k^ = d^k^ - 2, where d^k^ contains the degrees of nodes in T~&pi;~k.
6. If v^k = 0 ( T~&pi;~k is an optimal tour), or a stop criterion is satisfied,
then stop.
7. Choose a step size, t^k^.
8. Let &pi;~k+1~ = &pi;~k~ + t^k^v^k^
9. Let k = k + 1 and go to Step 2.


Having found a penalty vector &pi;, that maximizes w(&pi;), the transformation
given by &pi; of the original cost matrix C will often improve the &alpha;-measure
substantially. For example, for the 532-city problem every edge of the optimal
tour is among the 5 &alpha;-nearest neighbors for at least one of its endpoints.
The improvement of the &alpha;-measure reduces the average rank of the optimal
edges among the candidate edges from 2.1 to 1.7. This is close to the ideal
value of 1.5 (when every optimal edge has rank 1 or 2).


![QQ截图20170329093620]

[QQ截图20170329093620]: images/QQ-20170329093620.png "QQ截图20170329093620" { width:auto; max-width:90% }



The greatest advantage of the a-measure, however, is its usefulness for the
construction of the candidate set. By using the a-measure the cardinality of
the candidate set may generally be small without reducing the algorithm’s
ability to find short tours. Thus, **in all test problems the algorithm was able to
find optimal tours using as candidate edges only edges the 5 &alpha;-nearest edges
incident to each node. Most of the problems could even be solved when
search was restricted to only the 4 &alpha;-nearest edges**.

**The candidate edges of each node are sorted in ascending order of their &alpha;-values.
If two edges have the same a-value, the one with the smallest cost, c~ij~,
comes first. This ordering has the effect that candidate edges are considered
for inclusion in a tour according to their ‘promise’ of belonging to an optimal
tour. Thus, the &alpha;-measure is not only used to limit the search, but also to focus
the search on the most promising areas**.

To speed up the search even more, the algorithm uses a dynamic ordering of
the candidates. **Each time a shorter tour is found, all edges shared by this new
tour and the previous shortest tour become the first _two candidate edges_ for
their end nodes**.


## Breaking of tour edges

A candidate set is used to prune the search for edges, Y, to be included in a
tour. Correspondingly, the search of edges, X, to be excluded from a tour
may be restricted. In the actual implementation the following simple, yet very
effective, pruning rules are used:

* The first edge to be broken, x1, must not belong to the currently best
solution tour. When no solution tour is known, that is, during the
determination of the very first solution tour, x1 must not belong to
the minimum 1-tree.
* The last edge to be excluded in a basic move must not previously
have been included in the current chain of basic moves.

## Algorithm to find a initial tour

Experiments with various implementations of the new modified Lin-Kernighan
algorithm have shown that the quality of the final solutions does not
depend strongly on the initial tours. However, significant reduction in run
time may be achieved by choosing initial tours that are close to being optimal.

1. Choose a random node i.
2. Choose a node j, not chosen before, as follows:
If possible, choose j such that
(a) (i,j) is a candidate edge,
(b) a(i,j) = 0, and
(c) (i,j) belongs to the current best tour.
Otherwise, if possible, choose j such that (i,j) is a candidate edge.
Otherwise, choose j among those nodes not already chosen.
3. Let i = j. If not all nodes have been chosen, then go to Step 2.

This construction procedure is fast, and the diversity of initial solutions is
large enough for the edge exchange heuristics to find good final solutions.




