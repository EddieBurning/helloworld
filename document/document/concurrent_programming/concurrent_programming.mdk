Title         : CSAPP--concurrent Programming
Author        : Eddie
Logo          : True

[TITLE]

# difinition of concurrency
logical control flows are concurrent if they **_overlap in time_**.

# why is concurrency useful

* **_Accessing slow I/O devices_**. When an application is waiting for data to arrive
from a slow I/O device such as a disk, the kernel keeps the CPU busy by
running other processes. Individual applications can exploit concurrency in a
similar way by overlapping useful work with I/O requests.
* **_Interacting with humans_**.People who interact with computers demand the ability to perform multiple tasks at the same time. For example, they might want
to resize a window while they are printing a document. Modern windowing
systems use concurrency to provide this capability. Each time the user requests
some action (say, by clicking the mouse), a separate concurrent logical flow is
created to perform the action.
* **_Reducing latency by deferring work_**. Sometimes, applications can use concurrency to reduce the latency of certain operations by deferring other operations
and performing them concurrently. For example, a dynamic storage allocator
might reduce the latency of individual free operations by deferring coalescing to a concurrent “coalescing” flow that runs at a lower priority, soaking up
spare CPU cycles as they become available.
* **_Servicing multiple network clients_**. The iterative network servers that we studied in Chapter 11 are unrealistic because they can only service one client at
a time. Thus, a single slow client can deny service to every other client. For a
real server that might be expected to service hundreds or thousands of clients
per second, it is not acceptable to allow one slow client to deny service to the
others. **A better approach is to build a concurrent server that creates a separate
logical flow for each client**. This allows the server to service multiple clients
concurrently, and precludes slow clients from monopolizing the server.
* **_Computing in parallel on multi-core machines_**. Many modern systems are
equipped with multi-core processors that contain multiple CPUs. Applications that are partitioned into concurrent flows often run faster on multi-core
machines than on uniprocessor machines _because the flows execute in parallel
rather than being interleaved_.



# several approaches build concurrent programs
* **_Process_**

  With this approach, **each logical control flow is a process that is scheduled
  and  maintained by the kernel**. Since processes have separate virtual
  address spaces, flows that want to communicate with each other must use some
  kind of explicit interprocess communication (**_IPC_**) mechanism.
* **_I/O multiplexing_**.This is a form of concurrent programming where **applications
explicitly schedule their own logical flows in the context of a single process**.
Logical flows are modeled as state machines that the main program explicitly
transitions from state to state as a result of data arriving on file descriptors.
Since the program is a single process, all flows share the same address space.
* **_Threads_**. **_Threads are logical flows that run in the context of a single process
and are scheduled by the kernel_**. _You can think of threads as a hybrid of the
other two approaches_, scheduled by the kernel like process flows, and sharing
the same virtual address space like I/O multiplexing flows.

# Concurrent programming with process
## **pros & cons of process**
### advantages

Processes have a **clean model for sharing state information between parents and
children: file tables are shared and user address spaces are not**.Having separate
address spaces for processes is both an advantage and a disadvantage. It is impossible
for one process to accidentally overwrite the virtual memory of another
process, which eliminates a lot of confusing failures—an obvious advantage.
### disadvantages
* separate address spaces make it more **difficult for processes to share state information**.
* To share information, they must use **explicit IPC** (interprocess communications) mechanisms
* tend to be slower because the **overhead for process control and IPC is high**.

### different IPC
* **Unix Signals**
* **Socket**
* **Pipes**
* **Message queue**
* **FIFOs**
* **System V Shared Memory**
* **System V semaphores**

# Concurrent programming with I/O multiplexing
**_Suppose_** you are asked to write an echo server that can also respond to interactive
commands that the user types to standard input. In this case, the server must
respond to two independent I/O events: (1) a network client making a connection
request, and (2) a user typing a command line at the keyboard. Which event do we
wait for first? Neither option is ideal. If we are waiting for a connection request in
accept, then we cannot respond to input commands. Similarly, if we are waiting
for an input command in read, then we cannot respond to any connection requests.

One solution to this dilemma is a technique called **I/O multiplexing**. The basic
idea is to use the select function to ask the kernel to suspend the process, returning 
control to the application **only after one or more I/O events have occurred**, as
in the following examples:

* return when any descriptor in the set{0,4}is ready for reading.
* return when any descriptor in the set{1,2,7} is ready for writing.
* timeout if 152.13 seconds have elsapsed waiting for an I/O event to occur. 

Select is a complicated function with many different usage scenarios. We
will only discuss the first scenario: waiting for a set of descriptors to be ready for
reading.

## Concurrent Event-driven server based on I/O multiplexing
I/O multiplexing can be used as the basis for concurrent event-driven programs,
where flows make progress as a result of certain events. The general idea is to
model logical flows as state machines.

## pros & cons
* **pros**

 1. event-driven designs give programmers **_more control_** over the behavior of
   their programs than process-based designs.For example, we can imagine writing
    an event-driven concurrent server that gives preferred service to some clients,
which would be difficult for a concurrent server based on processes.
 2.  event-driven server based on I/O multiplexing **runs in the context of a single 
 process, and thus every logical flow has access to the entire address space** of the process. 
 This makes it easy to share data between flows.
 3. you can debug your concurrent server as you would any sequential program, using a familiar
debugging tool such as gdb, since it is a single process flow.
 4. vent-driven designs are often significantly more efficient than process-based designs 
 because **_they do not require a process context switch to schedule a new flow_**.

* **cons**
 1. A significant disadvantage of event-driven designs is **coding complexity**.Unfortunately, 
 **the complexity increases as the granularity of the concurrency decreases**.
  By granularity, we mean the number of instructions that each logical flow executes per 
  time slice. For instance, in our example concurrent server, the granularity of concurrency is the number of instructions required
to read an entire text line. As long as some logical flow is busy reading a text line,
no other logical flow can make progress. This is fine for our example, but it makes
our event-driver server vulnerable to a malicious client that sends only a partial
text line and then halts. Modifying an event-driven server to handle partial text
lines is a nontrivial task, but it is handled cleanly and automatically by a process-
based design.
 2. **can not make full use of multi-core processor**.
